# A Survey on IQA: Insights, Analysis, and Future Outlook

![img1](assets/Metric_Struct.jpg)

## ğŸ” Introduction

Image Quality Assessment (IQA) is a critical component of image-related technologies and plays a pivotal role in the advancement of image processing and computer vision. In recent years, the proliferation of novel training frameworks and machine learning models has given rise to a multitude of IQA methodologies. This survey conducts a comprehensive review of nearly 200 IQA-related publications, synthesizing key developments in the field and systematically categorizing existing approaches based on their underlying models, training frameworks, publication timelines, application scenarios, and academic impact. This structured analysis aims to facilitate a swift introduction for newcomers while providing seasoned researchers with a clearer perspective on the current state of the field. Moreover, we offer a critical evaluation of the advantages and limitations of various IQA methods and present our perspectives on future research directions. To complement the survey, this repository compiles both the IQA techniques discussed in the paper and other approaches that could not be included due to space constraints, thereby serving as a valuable resource to support further advancements in IQA research.<br>

å›¾åƒè´¨é‡è¯„ä¼°ï¼ˆIQAï¼‰åœ¨å›¾åƒç›¸å…³çš„æŠ€æœ¯ä¸­èµ·ç€éå¸¸é‡è¦çš„ä½œç”¨ï¼Œå¯¹äºå›¾åƒå¤„ç†å’Œè®¡ç®—æœºè§†è§‰é¢†åŸŸçš„æŠ€æœ¯å‘å±•æœ‰ç€æ·±è¿œçš„å½±å“ã€‚è¿‘å¹´æ¥ï¼Œéšç€æ–°å‹è®­ç»ƒæ¡†æ¶å’Œæœºå™¨å­¦ä¹ æ¨¡å‹çš„å‡ºç°ï¼Œè®¸å¤šIQAæ–¹æ³•æ¶Œç°å‡ºæ¥ã€‚æœ¬ç»¼è¿°é€šè¿‡è°ƒç ”æ¥è¿‘äºŒç™¾ç¯‡IQAç›¸å…³è®ºæ–‡ï¼Œæ€»ç»“äº†IQAå‘å±•ä¸­å€¼å¾—å…³æ³¨çš„å·¥ä½œï¼Œå¹¶æŒ‰ç…§ä¸åŒæ–¹æ³•æ‰€ç”¨çš„æ¨¡å‹ã€è®­ç»ƒæ¡†æ¶ã€å‘è¡¨æ—¶é—´ã€ä½¿ç”¨åœºæ™¯å’Œå½±å“åŠ›è¿›è¡Œäº†æ•´ç†ï¼Œä»¥æ–¹ä¾¿åˆå­¦è€…å¿«é€Ÿå…¥é—¨ã€èµ„æ·±ç ”ç©¶è€…æ›´å¥½åœ°äº†è§£é¢†åŸŸå‘å±•ç°çŠ¶ã€‚éšåï¼Œæˆ‘ä»¬å¯¹è¯¸å¤šIQAæ–¹æ³•çš„ä¼˜ç¼ºç‚¹è¿›è¡Œäº†åˆ†æï¼Œå¯¹IQAæ–¹æ³•çš„æœªæ¥å‘å±•æå‡ºäº†è‡ªå·±çš„è§è§£ã€‚æœ¬repositoryæ—¨åœ¨åˆ—å‡ºè®ºæ–‡ä¸­æåˆ°çš„ã€ä»¥åŠå—ç¯‡å¹…é™åˆ¶æœªæåˆ°çš„ç»å…¸IQAæ–¹æ³•ï¼Œé…åˆè®ºæ–‡é˜…è¯»ï¼Œä»¥ä¿ƒè¿›IQAæŠ€æœ¯çš„å‘å±•ã€‚

ğŸš€English Paper link: [A Survey on Image Quality Assessment: Insights, Analysis, and Future Outlook](https://arxiv.org/abs/2502.08540)<br>
ğŸš€Chinese Paper link: [åœ¨è®¡ç®—æœºè§†è§‰é¢†åŸŸä¸­ï¼Œæˆ‘ä»¬åº”è¯¥å¦‚ä½•è¯„ä¼°å›¾åƒè´¨é‡ï¼ˆä¸‡å­—é•¿æ–‡ï¼‰ï¼Ÿ](https://zhuanlan.zhihu.com/p/25680975953)

## Update Records

- ğŸ”¥ [March.2,2025] The chinese version of our survey has been released on zhihu.
- ğŸ”¥ [Feb.12,2025] The first version of our survey has been released on arXiv.

## ğŸ“œ Table of Contents

ğŸ“š Image Quality Assessment Methods
- ğŸ“— General Scene Methods
  - ğŸ“• Statistics Methods
    - ğŸ“˜ HVS-based Methods
    - ğŸ“˜ Transform Domain-based Methods
    - ğŸ“˜ Natural Scene Statistics-based Methods
  - ğŸ“• Machine Learning-based Methods
    - ğŸ“˜ Model-based Methods
      - ğŸ“™ Traditional Machine Learning Methods
      - ğŸ“™ CNN-based Methods
      - ğŸ“™ Transformer-based Methods
    - ğŸ“˜ Framework-based Methods

- ğŸ“— Specific Scene Methods
  - ğŸ“• Medical IQA
  - ğŸ“• IQA for Dehazing Algorithms
  - ğŸ“• Portrait Quality Assessment
  - ğŸ“• Specific Distortion
    - ğŸ“™ Blue
    - ğŸ“™ JPEG compression

## ğŸ“š Image Quality Assessment Methods

<table>
    <tr>
        <td>Metrics</td>
        <td>Introduction</td>
        <td>Paper name</td>
        <td>Citation number as of Feb.1,2025.</td>
        <td>Earliest publication time</td>
        <td><(Full-R, Reduced-R or Non-R)</td>
        <td>General Distortion or Specific Distortion <br>(å¦‚æœæ˜¯specific distortionï¼Œ<br>è¯·æŒ‡å‡ºå…·ä½“çš„distortionç§ç±»)</td>
        <td>æ‰€ä½¿ç”¨æ¡†æ¶<br>(å¦‚æœåˆ›æ–°ç‚¹åœ¨æ¡†æ¶ä¸Šï¼Œ<br>å¦‚æœæ˜¯ç»Ÿè®¡å­¦æ–¹æ³•åˆ™ä¸å¡«)</td>
        <td>æ‰€ä½¿ç”¨æ¨¡å‹<br>(å¦‚æœåˆ›æ–°ç‚¹åœ¨æ¨¡å‹ä¸Šï¼Œ<br>å¦‚æœæ˜¯ç»Ÿè®¡å­¦æ–¹æ³•åˆ™ä¸å¡«)</td>
    </tr>
    <tr>
        <td colspan="9"  align="center">HVS-Based Method</td>
    </tr>
    <tr>
        <td>signal-to-noise ratio (SNR)</td>
        <td>æœªè€ƒè™‘human visual system</td>
        <td></td>
        <td></td>
        <td></td>
        <td>FR</td>
        <td>G</td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>peak SNR (PSNR)</td>
        <td>æœªè€ƒè™‘human visual system</td>
        <td></td>
        <td></td>
        <td></td>
        <td>FR</td>
        <td>G</td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>mean squared error (MSE)</td>
        <td>æœªè€ƒè™‘human visual system</td>
        <td></td>
        <td></td>
        <td></td>
        <td>FR</td>
        <td>G</td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>structural similarity (SSIM)</td>
        <td>IQAå¸¸é’æ ‘ï¼Œä¸è€æˆ˜ç¥ï¼Œ20å¹´è¿‡å»äº†ï¼Œ</td>
        <td>Image quality assessment: From error visibility to structural similarity</td>
        <td>58352</td>
        <td>2004.04</td>
        <td>FR</td>
        <td>G</td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>multi-scale SSIM (MS-SSIM)</td>
        <td>ä¸ºäº†å°†SSIMç”¨äºä¸åŒåˆ†è¾¨ç‡çš„å›¾åƒï¼Œæå‡ºäº†MS-SSIM</td>
        <td>Multiscale structural similarity for image quality assessment</td>
        <td>8043</td>
        <td>2004.05</td>
        <td>FR</td>
        <td>G</td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>universal image quality index(UQI)</td>
        <td>é€šè¿‡å°†ä»»ä½•å›¾åƒå¤±çœŸå»ºæ¨¡ä¸ºä¸‰ä¸ªå› ç´ çš„ç»„åˆï¼šç›¸å…³æ€§æŸå¤±ã€äº®åº¦å¤±çœŸå’Œå¯¹æ¯”åº¦å¤±çœŸ</td>
        <td>A universal image quality index</td>
        <td>7419</td>
        <td>2002.03</td>
        <td>FR</td>
        <td>G</td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>feature similarity index (FSIM)</td>
        <td>FSIMä¸»è¦ä¾èµ–äºç›¸ä½ä¸€è‡´æ€§ï¼ˆPCï¼‰å’Œæ¢¯åº¦å¹…åº¦ï¼ˆGMï¼‰è¿™ä¸¤ä¸ªä½çº§ç‰¹å¾ï¼Œé€šè¿‡ç”Ÿæˆå±€éƒ¨ç›¸ä¼¼æ€§å›¾å¹¶ä½¿ç”¨ç›¸ä½ä¸€è‡´æ€§ä½œä¸ºåŠ æƒå‡½æ•°ï¼Œå°†å±€éƒ¨è´¨é‡æ•´åˆæˆå•ä¸€è¯„åˆ†</td>
        <td>FSIM: A feature similarity index for image quality assessment</td>
        <td>5512</td>
        <td>2011.01</td>
        <td>FR</td>
        <td>G</td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>Visual information fidelity(VIF)</td>
        <td> 1.VIF algorithm models natural images in wavelet domain using GSM model.<br>2.Distortion is modeled in the wavelet domain as signal attenuation and additive noise.</td>
        <td>Image information and visual quality</td>
        <td>4861</td>
        <td>2006.02</td>
        <td>FR</td>
        <td>G</td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>most apparent distortion (MAD)</td>
        <td>MADæ–¹æ³•é€šè¿‡åŠ æƒå‡ ä½•å¹³å‡çš„æ–¹å¼ç»“åˆæ£€æµ‹åŸºç¡€ç­–ç•¥ã€å¤–è§‚åŸºç¡€ç­–ç•¥è¿™ä¸¤ç§ç­–ç•¥çš„è¾“å‡ºï¼Œæœ€ç»ˆå¾—å‡ºå›¾åƒçš„æ€»ä½“è´¨é‡è¯„åˆ†ã€‚åœ¨é«˜å¤±çœŸå›¾åƒä¸­ï¼Œå¤–è§‚åŸºç¡€è´¨é‡çš„æƒé‡æ›´å¤§ï¼›è€Œåœ¨è¿‘é˜ˆå€¼å¤±çœŸçš„é«˜è´¨é‡å›¾åƒä¸­ï¼Œæ£€æµ‹åŸºç¡€è´¨é‡çš„æƒé‡åˆ™æ›´å¤§ã€‚</td>
        <td>Most apparent distortion: Full-reference image quality assessment and the role of strategy</td>
        <td>2250</td>
        <td>2010.01</td>
        <td>FR</td>
        <td>G</td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>Gradient magnitude similarity deviation (GMSD)</td>
        <td>GMSDåŸºäºå›¾åƒæ¢¯åº¦ä¿¡æ¯ï¼Œé€šè¿‡è®¡ç®—å‚è€ƒå›¾åƒä¸å¤±çœŸå›¾åƒçš„æ¢¯åº¦å¹…å€¼ç›¸ä¼¼æ€§æ¥è¯„ä¼°è´¨é‡ï¼Œä½¿ç”¨æ ‡å‡†å·®ä½œä¸ºæ± åŒ–ç­–ç•¥</td>
        <td>Gradient magnitude similarity deviation: A highly efficient perceptual image quality index</td>
        <td>1716</td>
        <td>2013.12</td>
        <td>FR</td>
        <td>G</td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>Visual SNR (VSNR)</td>
        <td>è€ƒè™‘human visual system<br>  (HVS)</td>
        <td>VSNR: A wavelet-based visual signal-to-noise ratio for natural images</td>
        <td>1578</td>
        <td>2007.08</td>
        <td>FR</td>
        <td>G</td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>information content weighted SSIM measure (IW-SSIM)</td>
        <td>IW-MSEä½¿ç”¨äº†å¤šå°ºåº¦ä¿¡æ¯å†…å®¹æƒé‡æ¥è®¡ç®—å±€éƒ¨å‡æ–¹è¯¯å·®ï¼ŒIW-PSNRåˆ™æ˜¯åœ¨IW-MSEçš„åŸºç¡€ä¸Šå°†å‡æ–¹è¯¯å·®è½¬æ¢ä¸ºå³°å€¼ä¿¡å™ªæ¯”</td>
        <td>Information content weighting for perceptual image quality assessment</td>
        <td>1533</td>
        <td>2010.11</td>
        <td>FR</td>
        <td>G</td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>9</td>
        <td>é¦–æ¬¡æå‡ºè¿™æ ·ä¸€ä¸ªè§‚ç‚¹ï¼Œä¿ƒè¿›äº†SSIMçš„å‡ºç°ï¼š<br>The main function of the human eyes is to extract structural information from the viewing field, and the human visual system is highly adapted for this purpose</td>
        <td>Why is image quality assessment so difficult?</td>
        <td>1319</td>
        <td>2002.05</td>
        <td>FR</td>
        <td>G</td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>visual saliency-induced index(VSI)</td>
        <td>VSIç»“åˆè§†è§‰æ˜¾è‘—æ€§ã€æ¢¯åº¦æ¨¡å’Œè‰²åº¦ä¿¡æ¯ï¼Œåˆ©ç”¨è§†è§‰æ˜¾è‘—æ€§ä½œä¸ºå±€éƒ¨è´¨é‡ç‰¹å¾å’ŒåŠ æƒå‡½æ•°åæ˜ å›¾åƒä¸­ä¸åŒåŒºåŸŸçš„é‡è¦æ€§</td>
        <td>VSI: A visual saliency-induced index for perceptual image quality assessment</td>
        <td>1082</td>
        <td>2014.08</td>
        <td>FR</td>
        <td>G</td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>GSIM</td>
        <td>åŸºäºSSIM, æå‡ºçš„åŸºäºæ¢¯åº¦ç›¸ä¼¼æ€§çš„å›¾åƒè´¨é‡è¯„ä¼°æ–¹æ³•ï¼Œå……åˆ†è€ƒè™‘äº†å›¾åƒçš„æ¢¯åº¦ç‰¹å¾ï¼Œå°¤å…¶æ˜¯è¾¹ç¼˜ã€çº¹ç†å’Œç»†èŠ‚éƒ¨åˆ†</td>
        <td>Image quality assessment based on gradient similarity</td>
        <td>872</td>
        <td>2011.11</td>
        <td>FR</td>
        <td>G</td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>complex wavelets-SSIM (CW-SSIM)</td>
        <td>ç»“æ„ç›¸ä¼¼æ€§è™½ç„¶èƒ½å¤§è‡´ç¬¦åˆäººç±»çš„è§†è§‰ç³»ç»Ÿçš„æ„Ÿå—ï¼Œä½†è‹¥å›¾ç‰‡é‡åˆ°å‡ ä½•ä¸Šçš„è½¬æ¢ï¼Œä¾‹å¦‚å¹³ç§»ã€æ—‹è½¬ä¸ç¼©æ”¾æ—¶ï¼Œç»“æ„ç›¸ä¼¼æ€§ä¼šæ— æ³•æ­£ç¡®æè¿°ä¸¤å¼ å›¾ç‰‡çš„ç›¸ä¼¼ç¨‹åº¦</td>
        <td>Translation insensitive image similarity in complex wavelet domain</td>
        <td>455</td>
        <td>2005.05</td>
        <td>FR</td>
        <td>G</td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>HVS-based peak SNR (PSNR-HVS)</td>
        <td>è€ƒè™‘human visual system</td>
        <td>A new full-reference quality metrics based on HVS</td>
        <td>433</td>
        <td>2006.01</td>
        <td>FR</td>
        <td>G</td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>RR-SSIM</td>
        <td>ä¸ºäº†åœ¨è¯„ä¼°å›¾åƒè´¨é‡çš„è¿‡ç¨‹ä¸­è€ƒè™‘psychophysical visual masking effectï¼Œä½¿ç”¨DNTï¼ˆDivisive Normalization Transformï¼Œé™¤æ³•å½’ä¸€åŒ–å˜æ¢ï¼‰æå–å›¾åƒä¿¡æ¯ï¼Œå¹¶ç”¨äºä¼°è®¡SSIMã€è¯„ä»·å›¾åƒè´¨é‡ã€‚<br>DNT:DNT åœ¨å¤šå°ºåº¦å’Œå¤šæ–¹å‘çš„å°æ³¢å˜æ¢åŸºç¡€ä¸Šè¿›è¡Œçš„ï¼Œèƒ½å¤Ÿæå–å›¾åƒçš„å±€éƒ¨ç»“æ„ä¿¡æ¯ï¼ŒåŒæ—¶è€ƒè™‘åˆ°ç›¸é‚»åƒç´ æˆ–ç³»æ•°ä¹‹é—´çš„ç›¸äº’å½±å“,é€šè¿‡å°†æ¯ä¸ªç³»æ•°é™¤ä»¥ä¸€ä¸ªåŸºäºå…¶é‚»åŸŸç³»æ•°çš„å±€éƒ¨èƒ½é‡åº¦é‡ï¼Œä»è€Œå®ç°å½’ä¸€åŒ– </td>
        <td>Reduced-reference image quality assessment by structural similarity estimation</td>
        <td>320</td>
        <td>2012.08</td>
        <td>RR</td>
        <td>G</td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>PSIM</td>
        <td>åŸºäºSSIM, é€šè¿‡èåˆå›¾åƒçš„å¾®è§‚ç»“æ„å’Œå®è§‚ç»“æ„ä¿¡æ¯ï¼Œæä¾›äº†ä¸€ç§æ—¢ç²¾ç¡®åˆé«˜æ•ˆçš„å›¾åƒè´¨é‡è¯„ä¼°æ¡†æ¶</td>
        <td>A fast reliable image quality predictor by fusing micro- and macro-structures</td>
        <td>238</td>
        <td>2017.05</td>
        <td>FR</td>
        <td>G</td>
        <td>PSIM</td>
        <td></td>
    </tr>
    <tr>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td colspan="9" align="center">Transform Domain-Based Method</td>
    </tr>
    <tr>
        <td>Blind Image Integrity Notator using DCT Statisticsï¼ˆBLIINDS-IIï¼‰</td>
        <td>natural scene statistics (NSS)-based NR metrics</td>
        <td>Blind image quality assessment: A natural scene statistics approach in the DCT domain</td>
        <td>1912</td>
        <td>2012.03</td>
        <td>NR</td>
        <td>G</td>
        <td>NSS+DCT</td>
        <td></td>
    </tr>
    <tr>
        <td>87</td>
        <td>ä½¿ç”¨wavelet transformï¼Œåˆ©ç”¨wavelet coefficientséšç€å›¾åƒå¤±çœŸå‘ç”Ÿçš„å˜åŒ–è¿›è¡Œè¯„ä¼°</td>
        <td>Reduced-reference image quality assessment using a wavelet-domain natural image statistic model</td>
        <td>600</td>
        <td>2005.03</td>
        <td>RR</td>
        <td>G</td>
        <td></td>
        <td>å°æ³¢å˜æ¢</td>
    </tr>
    <tr>
        <td>74</td>
        <td>å¦ä¸€ç§åŸºäºSVDçš„åº¦é‡ï¼Œä½œè€…é¦–å…ˆè®¡ç®—å‚è€ƒå›¾åƒå—å’Œå¤±çœŸå›¾åƒå—çš„å¥‡å¼‚å€¼ä¹‹é—´çš„è·ç¦»ï¼Œç„¶åè®¡ç®—æ¯ä¸ªå—çš„å…¨å±€å€¼ä»¥è¡¨ç¤ºæœ€ç»ˆè´¨é‡ã€‚</td>
        <td>An SVD-based grayscale image quality measure for local and global assessment</td>
        <td>482</td>
        <td>2006.02</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>SFF</td>
        <td>ç¨€ç–ç‰¹å¾ä¿çœŸåº¦ï¼ˆSFFï¼‰åº¦é‡;<br>è¡¥å……ï¼šè¿™ç¯‡paperä½¿ç”¨ICAæ¥æ¨¡ä»¿çš®å±‚ç®€å•ç»†èƒçš„è§†è§‰ç³»ç»Ÿï¼Œå¾ˆä¼šè®²æ•…äº‹å•Šã€‚</td>
        <td>Sparse feature fidelity for perceptual image quality assessment</td>
        <td>180</td>
        <td>2013.06</td>
        <td>FR</td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>80</td>
        <td>åœ¨å±€éƒ¨å›¾åƒå—ä¸­ä½¿ç”¨äº†ç¨€ç–è¡¨ç¤ºï¼Œå¹¶è®¡ç®—äº†å‚è€ƒå’Œå¤±çœŸå—çš„ç¨€ç–è¡¨ç¤ºä¹‹é—´çš„å·®å¼‚ï¼Œä½œä¸ºè´¨é‡å›¾ã€‚é‡‡ç”¨æ ¸å²­å›å½’ï¼ˆKRRï¼‰å°†å±€éƒ¨è´¨é‡èåˆåˆ°æœ€ç»ˆçš„è´¨é‡åˆ†æ•°ä¸­ã€‚</td>
        <td>Image quality assessment: a sparse learning way</td>
        <td>55</td>
        <td>2015.07</td>
        <td>FR</td>
        <td>G</td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>QASD</td>
        <td>ä¸€ç§åŸºäºç¨€ç–è¡¨ç¤ºçš„è‡ªé€‚åº”å­å­—å…¸ï¼ˆQASDï¼‰å›¾åƒè´¨é‡æŒ‡æ•°</td>
        <td>Sparse representation-based image quality index with adaptive sub-dictionaries</td>
        <td>50</td>
        <td>2016.06</td>
        <td>FR</td>
        <td>G</td>
        <td>QASD</td>
        <td></td>
    </tr>
    <tr>
        <td>81</td>
        <td>é¦–å…ˆä¸ºç¨€ç–ç¼–ç éƒ¨ç½²äº†å‚…ç«‹å¶åŸºï¼Œç„¶åå¯¹ç¨€ç–ç³»æ•°çš„å¹…åº¦è¿›è¡Œäº†æ’åºï¼Œæœ€åè¯„ä¼°äº†å‚è€ƒå›¾åƒå’Œå¤±çœŸå›¾åƒçš„æ’åºç³»æ•°ä¹‹é—´çš„å¯¹åº”æ€§ã€‚</td>
        <td>From sparse coding significance to perceptual quality: a new approach for image quality assessment</td>
        <td>50</td>
        <td>2017.11</td>
        <td>FR</td>
        <td>G</td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td colspan="9" align="center">NSS-Based Method</td>
    </tr>
    <tr>
        <td>160</td>
        <td>æå‡ºä¸€ä¸ªå¤§è§„æ¨¡ä¸»è§‚è´¨é‡è¯„ä¼°æ•°æ®é›†ï¼Œå¯¹ 10 ç§ä¸»æµå…¨å‚è€ƒå›¾åƒè´¨é‡è¯„ä¼°ç®—æ³•è¿›è¡Œç³»ç»Ÿå¯¹æ¯”ï¼Œå‘ç°å…¶æ•ˆæœä¸ä½³ï¼Œæœ‰å¾ˆå¤§è¿›æ­¥ç©ºé—´</td>
        <td>A statistical evaluation of recent full reference image quality assessment algorithms</td>
        <td>3526</td>
        <td>2006.10</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>IFC</td>
        <td></td>
        <td>An information fidelity criterion for image quality assessment using natural scene statistics</td>
        <td>1721</td>
        <td>2005.11</td>
        <td>FR</td>
        <td>G</td>
        <td></td>
        <td>NSS</td>
    </tr>
    <tr>
        <td>Tone-mapped images quality indexâ€”TMQIï¼ˆè°ƒæ•´åçš„è‰²è°ƒæ˜ å°„å›¾åƒè´¨é‡æŒ‡æ•°ï¼‰</td>
        <td>æ˜¯ä¸€ç§ç”¨äºè¯„ä¼°è‰²è°ƒæ˜ å°„å›¾åƒè´¨é‡çš„å…¨å‚è€ƒå›¾åƒè´¨é‡è¯„ä¼°ï¼ˆFR-IQAï¼‰æ–¹æ³•ã€‚å®ƒç»“åˆäº†å¤šå°ºåº¦ç»“æ„ä¿çœŸåº¦æµ‹é‡å’Œç»Ÿè®¡è‡ªç„¶åº¦æµ‹é‡ï¼Œä»¥å®¢è§‚åœ°è¯„ä¼°ç”±è‰²è°ƒæ˜ å°„ç®—å­ï¼ˆTMOï¼‰ç”Ÿæˆçš„ä½åŠ¨æ€èŒƒå›´ï¼ˆLDRï¼‰å›¾åƒçš„è´¨é‡</td>
        <td>Objective quality assessment of tone-mapped images</td>
        <td>720</td>
        <td>2012.10</td>
        <td>FR</td>
        <td>G</td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>36</td>
        <td>é€šè¿‡å›¾åƒçš„anisotropyè¯„ä¼°å…¶è´¨é‡ã€‚</td>
        <td>Blind image quality assessment through anisotropy</td>
        <td>397</td>
        <td>2007.09</td>
        <td></td>
        <td>G</td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>Dynamic range independent quality measureâ€”DRIMï¼ˆåŠ¨æ€èŒƒå›´ç‹¬ç«‹åº¦é‡ï¼‰</td>
        <td>test image å’Œ reference imageå¯èƒ½scaleä¸åŒï¼Œè¯¥æ–¹æ³•åœ¨è¯„ä¼°è´¨é‡æ—¶å¯ä»¥æ— è§†å°ºåº¦scale</td>
        <td>Dynamic range independent image quality assessment</td>
        <td>371</td>
        <td>2008.08</td>
        <td>FR</td>
        <td>G</td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td colspan="9" align="center">Traditional Machine Learning  Method</td>
    </tr>
    <tr>
        <td>The blind/referenceless image spatial quality evaluator (BRISQUE)</td>
        <td>natural scene statistics (NSS)-based NR metrics;</td>
        <td>No-reference image quality assessment in the spatial domain</td>
        <td>5733</td>
        <td>2012.08</td>
        <td>NR</td>
        <td>G</td>
        <td>Natural Scene Statisticsï¼ˆNSSï¼‰</td>
        <td></td>
    </tr>
    <tr>
        <td>distortion identification-based image verity and integrity evaluation(DIIVINE)</td>
        <td>natural scene statistics (NSS)-based NR metrics</td>
        <td>Blind image quality assessment: From natural scene statistics to perceptual quality</td>
        <td>2003</td>
        <td>2011.01</td>
        <td>NR</td>
        <td>G</td>
        <td>Natural Scene Statisticsï¼ˆNSSï¼‰</td>
        <td></td>
    </tr>
    <tr>
        <td>Blind Image Integrity Notator using DCT Statisticsï¼ˆBLIINDS-IIï¼Œå’Œtransform domain-based methodå†²çªï¼‰</td>
        <td>natural scene statistics (NSS)-based NR metrics</td>
        <td>Blind image quality assessment: A natural scene statistics approach in the DCT domain</td>
        <td>1912</td>
        <td>2012.03</td>
        <td>NR</td>
        <td>G</td>
        <td>NSS+DCT</td>
        <td></td>
    </tr>
    <tr>
        <td>IL-NIQE</td>
        <td>ä½¿ç”¨natural scene statistics (NSS)-based methodsæŠ½å–reliable featuresï¼›<br>ä»äº’è”ç½‘ä¸Šç­›é€‰90é•¿é«˜è´¨é‡å›¾ç‰‡ï¼Œä»¥æ­¤è®­ç»ƒMVGæ¨¡å‹ï¼Œé€šè¿‡è®¡ç®—å›¾ç‰‡ä¸åŸå§‹MVGæ¨¡å‹çš„è·ç¦»ç»™å›¾ç‰‡æ‰“åˆ†ã€‚<br>è¡¥å……ï¼š<br>ä½¿ç”¨ä»å›¾åƒä¸­æå–çš„NSSç‰¹å¾å­¦ä¹ ä¸€ä¸ªmultivariate Gaussian (MVG) modelï¼Œç”¨äºè¯„ä¼°å›¾åƒè´¨é‡</td>
        <td>A feature-enriched completely blind image quality evaluator</td>
        <td>1214</td>
        <td>2015.04</td>
        <td>NR</td>
        <td>G</td>
        <td>MVG</td>
        <td></td>
    </tr>
    <tr>
        <td>130</td>
        <td>NRç±»å‹ï¼ŒHVS features such as edge ampli-tude,edge length,background activity and back groundluminance   are used.These features along with MOS of images are fed to extreme machine learning(ELM)[27]algorithm to obtain a func-tional relationship.<br>è¡¥å……ï¼š<br>æå–ç©ºé—´å’Œé¢‘è°±ç†µç‰¹å¾ï¼ˆSpatial and Spectral Entropy Featuresï¼‰ï¼Œå¹¶ä½¿ç”¨å•éšè—å±‚å‰é¦ˆç¥ç»ç½‘ç»œæé™å­¦ä¹ æœºï¼ˆExtreme Learning Machine, ELMï¼‰ç¡®å®šå¤±çœŸç±»å‹ï¼Œå¹¶ä¼°è®¡å¤±çœŸç¨‹åº¦ã€‚</td>
        <td>No-reference image quality assessment using modified extreme learning machine classifier</td>
        <td>292</td>
        <td>2009.03</td>
        <td>NR</td>
        <td>G(å…·ä½“æœ‰äº”ç§)</td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>multi-method fusion (MMF)</td>
        <td>ä¸åŒçš„è´¨é‡è¯„ä¼°æ–¹æ³•å„æœ‰ä¼˜åŠ£ï¼Œä¸å¦¨å°†å…¶å…¨éƒ¨æ•´åˆèµ·æ¥ï¼Œéšåé‡‡ç”¨æ”¯æŒå‘é‡å›å½’ï¼ˆSVRï¼‰æ¥æ•´åˆå¤šä¸ªè´¨é‡æŒ‡æ ‡çš„å¾—åˆ†</td>
        <td>Image quality assessment using multi-method fusion</td>
        <td>216</td>
        <td>2012.12</td>
        <td>FR</td>
        <td>G</td>
        <td></td>
        <td>SVR</td>
    </tr>
    <tr>
        <td>73</td>
        <td>ä½¿ç”¨äº†åŸºäºå¥‡å¼‚å€¼åˆ†è§£ï¼ˆSVDï¼‰çš„ç‰¹å¾ï¼Œå¹¶å°†æ”¯æŒå‘é‡å›å½’ï¼ˆSVRï¼‰ç”¨ä½œç‰¹å¾èåˆå·¥å…·ã€‚</td>
        <td>Objective image quality assessment based on support vector regression</td>
        <td>200</td>
        <td>2010.01</td>
        <td>èƒ½å¤Ÿé¢„æµ‹å…¶å†…å®¹å’Œå¤±çœŸç±»å‹æœªå‡ºç°åœ¨è®­ç»ƒé›†ä¸­çš„å›¾åƒçš„æ„ŸçŸ¥è´¨é‡</td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>SVDR</td>
        <td>ä½¿ç”¨ï¼š<br>å¥‡å¼‚å€¼åˆ†è§£SVD<br>è¿›è¡Œç‰¹å¾æå–ï¼Œ<br>ä½¿ç”¨<br>æœºå™¨å­¦ä¹ æ”¯æŒå‘é‡é€€åŒ–SVDR<br>è¿›è¡Œç‰¹å¾èåˆã€‚</td>
        <td>SVD-based quality metric for image and video using machine learning</td>
        <td>184</td>
        <td>2011.09</td>
        <td>FR</td>
        <td>G</td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>93</td>
        <td>Inspired by the hierarchical and trichromatic properties of human vision, we extract both the frequency and spatial-frequency features from all three YCbCr channels to describe a natural image. Then, a KNN(k-nearest-neighbor)-based LT(Label Transfer) model is used to estimate the query imageâ€™s quality.</td>
        <td>Blind Image Quality Assessment Based on Multichannel Feature Fusion and Label Transfer</td>
        <td>176</td>
        <td>2015.03</td>
        <td>NR</td>
        <td>G</td>
        <td></td>
        <td>KNN</td>
    </tr>
    <tr>
        <td>ParaBoost</td>
        <td>Paraboost method based on SVR</td>
        <td>A paraboost method to image quality assessment</td>
        <td>86</td>
        <td>2015.12</td>
        <td>FR</td>
        <td>G</td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td colspan="9" align="center">CNN-Based  Method</td>
    </tr>
    <tr>
        <td>163</td>
        <td>æå‡ºäº†ä¸€ä¸ªæ–°çš„äººç±»æ„ŸçŸ¥ç›¸ä¼¼æ€§åˆ¤æ–­æ•°æ®é›†ã€‚<br>è¯„ä¼°äº†ä¸åŒæ¶æ„å’Œä»»åŠ¡ä¸­çš„æ·±åº¦ç‰¹å¾ï¼Œå¹¶å°†å®ƒä»¬ä¸ç»å…¸æŒ‡æ ‡è¿›è¡Œæ¯”è¾ƒã€‚å‘ç°åœ¨è¯¥æ•°æ®é›†ä¸Šæ·±åº¦ç‰¹å¾çš„è¡¨ç°å¤§å¤§ä¼˜äºä»¥å‰çš„æ‰€æœ‰æŒ‡æ ‡ã€‚</td>
        <td>The Unreasonable Effectiveness of Deep Features as a Perceptual Metric</td>
        <td>12353</td>
        <td>2018.01</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>IQA-CNN</td>
        <td>A pioneering work.</td>
        <td>Convolutional neural networks for no-reference image quality assessment</td>
        <td>1406</td>
        <td>2014.09</td>
        <td>NR</td>
        <td>G</td>
        <td></td>
        <td>CNN</td>
    </tr>
    <tr>
        <td>32</td>
        <td>é€šè¿‡Siameseç»“æ„å¤„ç†å‚è€ƒå›¾åƒå’Œå¤±çœŸå›¾åƒï¼Œå®ç°ç‰¹å¾çš„å…±äº«å’Œæ¯”è¾ƒï¼›<br>æå‡ºä¸‰ä¸ªç½‘ç»œï¼ŒWaDIQaM - NRï¼ŒDIQaM - NRï¼ŒWaDIQaM-FR</td>
        <td>Deep neural networks for no-reference and full-reference image quality assessment</td>
        <td>1229</td>
        <td>2017.10</td>
        <td>FR&amp;NR</td>
        <td>G</td>
        <td></td>
        <td>DNN</td>
    </tr>
    <tr>
        <td>DISTS</td>
        <td>å¤§å¤šCNN-based FR IQAæ–¹æ³•å¯¹å›¾åƒçš„texture similarityè¿‡äºæ•æ„Ÿï¼ŒDISTSä½œä¸ºthe first of its kind with built-in tolerance to texture resamplingï¼ŒåŸºäºVGGæå–texture informationï¼Œå°†structure and texture informationå¾ˆå¥½åœ°ç»“åˆäº†èµ·æ¥</td>
        <td>Image Quality Assessment: Unifying Structure and Texture Similarity</td>
        <td>847</td>
        <td>2020.12</td>
        <td>FR</td>
        <td>G</td>
        <td></td>
        <td>VGG</td>
    </tr>
    <tr>
        <td>BIQA by a Self-Adaptive Hyper Network</td>
        <td>æå‡ºä¸€ç§åŸºäºè‡ªé€‚åº”è¶…ç½‘ç»œçš„å›¾åƒè´¨é‡è¯„ä¼°æ¨¡å‹ï¼Œé€šè¿‡å°† IQA è¿‡ç¨‹åˆ†ä¸ºå†…å®¹ç†è§£ã€æ„ŸçŸ¥è§„åˆ™å­¦ä¹ å’Œè´¨é‡é¢„æµ‹ä¸‰ä¸ªé˜¶æ®µï¼Œå®ç°äº†å¯¹è‡ªç„¶åœºæ™¯å›¾åƒè´¨é‡çš„æœ‰æ•ˆè¯„ä¼°ã€‚</td>
        <td>Blindly Assess Image Quality in the Wild Guided by a Self-Adaptive Hyper Network</td>
        <td>667</td>
        <td>2020.06</td>
        <td>NR</td>
        <td>G</td>
        <td>Self-Adaptive Hyper Network</td>
        <td></td>
    </tr>
    <tr>
        <td>MEON</td>
        <td>DNN methods for BIQA;<br>DNN-based BIQA using deep features and quality prediction together;<br>predicting image quality scores;<br>The image-input methods;<br>Expanding distorted images</td>
        <td>End-to-end blind image quality assessment using deep neural networks</td>
        <td>593</td>
        <td>2017.11</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>RankIQA</td>
        <td></td>
        <td>RankIQA: Learning from rankings for no-reference image quality assessment</td>
        <td>556</td>
        <td>2017.07</td>
        <td>NR</td>
        <td>G</td>
        <td></td>
        <td>CNN</td>
    </tr>
    <tr>
        <td>BIECON</td>
        <td>ä½¿ç”¨FR-IQAæŒ‡æ ‡ç”Ÿæˆçš„å±€éƒ¨è´¨é‡å›¾ä½œä¸ºä¸­é—´å›å½’ç›®æ ‡ </td>
        <td>Fully deep blind image quality predictor</td>
        <td>487</td>
        <td>2016.12</td>
        <td>NR</td>
        <td>G</td>
        <td></td>
        <td>CNN</td>
    </tr>
    <tr>
        <td>101</td>
        <td>DNN methods for BIQA;<br>DNN-based BIQA using deep features and quality prediction together;<br>predicting image quality categories<br>DNNç›´æ¥æ ¹æ®è´¨é‡ç»™å›¾åƒåˆ†ç­‰çº§</td>
        <td>Blind image quality assessment via deep learning</td>
        <td>433</td>
        <td>2014.08</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>DeepBIQ</td>
        <td>DNN methods for BIQA;<br>DNN-based BIQA using deep features and quality prediction together;<br>predicting image quality categories<br>DNNç›´æ¥æ ¹æ®è´¨é‡ç»™å›¾åƒåˆ†ç­‰çº§</td>
        <td>On the use of deep learning for blind image quality assessment</td>
        <td>396</td>
        <td>2016.02</td>
        <td>NR</td>
        <td>G</td>
        <td></td>
        <td>CNN</td>
    </tr>
    <tr>
        <td>deep image quality assessment (DeepQA)</td>
        <td></td>
        <td>Deep learning of human visual sensitivity in image quality assessment framework</td>
        <td>304</td>
        <td>2017.10</td>
        <td>FR</td>
        <td>G</td>
        <td></td>
        <td>DNN+HVS</td>
    </tr>
    <tr>
        <td>DeepSim</td>
        <td>ä¸€ç§åŸºäºæ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNï¼‰çš„æ·±åº¦ç›¸ä¼¼æ€§æŒ‡æ•°ï¼ˆDeepSimï¼‰ã€‚<br>è¡¥å……ï¼šä»¥é¢„è®­ç»ƒå¥½çš„VGGnetä¸ºåŸºç¡€</td>
        <td>DeepSim: deep similarity for image quality assessment</td>
        <td>190</td>
        <td>2017.09</td>
        <td>FR</td>
        <td>G</td>
        <td></td>
        <td>VGG</td>
    </tr>
    <tr>
        <td>95</td>
        <td>DNN methods for BIQA;<br>SVR-based BIQA using deep features extracted by DNN;<br>Extracting from low-level features;</td>
        <td>Blind image quality assessment using semi-supervised rectifier networks</td>
        <td>126</td>
        <td>2014.09</td>
        <td>NR</td>
        <td>G</td>
        <td>Semi-supervised Learning</td>
        <td>Rectifier Neural Network</td>
    </tr>
    <tr>
        <td>Re-IQA</td>
        <td>æå‡ºä¸€ç§ç”±Content-Aware Encoderå’ŒQuality-Aware Encoderä¸¤ä¸ªencoderç»„æˆçš„Mixture of Expertsæ¡†æ¶ï¼Œå…¶ä¸­ç”¨åˆ°äº†contrastive learningè¿›è¡Œè®­ç»ƒã€‚</td>
        <td>Re-IQA: Unsupervised Learning for Image Quality Assessment in the Wild</td>
        <td>84</td>
        <td>2023.04</td>
        <td>NR</td>
        <td>G</td>
        <td>Mixture of Experts, contrastive learning</td>
        <td></td>
    </tr>
    <tr>
        <td>Multi-Pooled Inception Features for NR IQA</td>
        <td>ä½¿ç”¨Inceptionæ¨¡å—çš„ç‰¹ç‚¹ï¼ˆåŒ…å«å¤šç§å·ç§¯æ ¸å¤§å°çš„æ»¤æ³¢å™¨ï¼‰ï¼Œå¹¶è¡Œå¤„ç†ä¸åŒscaleçš„è§†è§‰ä¿¡æ¯ã€‚</td>
        <td>Multi-Pooled Inception Features for No-Reference Image Quality Assessment</td>
        <td>44</td>
        <td>2020.02</td>
        <td>NR</td>
        <td>G</td>
        <td></td>
        <td>Inception</td>
    </tr>
    <tr>
        <td>IQMA Network(21 NTIRE public leaderboardå† å†›)</td>
        <td>ä¸ºäº†è¯„ä»·GAN ç”Ÿæˆçš„å›¾åƒä¸­é€šå¸¸å…·æœ‰çš„çœ‹ä¼¼çœŸå®å´è™šå‡çš„ç»†èŠ‚å’Œçº¹ç†ï¼Œæå‡ºåŒåˆ†æ”¯å¤šå°ºåº¦çš„FR-IQAæ–¹æ³•ã€‚</td>
        <td>IQMA Network: Image Quality Multi-Scale Assessment Network</td>
        <td>27</td>
        <td>2021.06</td>
        <td>FR</td>
        <td>G</td>
        <td>multi-scale</td>
        <td></td>
    </tr>
    <tr>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td colspan="9" align="center">Transformer-Based  Method</td>
    </tr>
    <tr>
        <td>MUSIQ</td>
        <td>ä¸€ä¸ªé€šè¿‡Multi-scale Patch Embeddingæ–¹æ³•å¤„ç†å¤šç§sizeçš„å›¾ç‰‡ï¼Œè€Œä¸ç”¨å¯¹å…¶è¿›è¡Œè£å‰ªã€å¼ºè¡Œè°ƒæ•´å¤§å°ï¼Œä»è€Œå¯¹åˆ†è¾¨ç‡å’Œçºµæ¨ªæ¯”æ•æ„Ÿçš„åœºæ™¯ä¸‹çš„IQAæœ‰å¥‡æ•ˆçš„MUSIQæ–¹æ³•ã€‚</td>
        <td>MUSIQ: Multi-Scale Image Quality Transformer</td>
        <td>601</td>
        <td>2021.08</td>
        <td>NR</td>
        <td>G</td>
        <td>Transformer</td>
        <td></td>
    </tr>
    <tr>
        <td>ViT with relative ranking and self-consistency</td>
        <td>é€šè¿‡ä¿®æ”¹loss functionçš„æ–¹å¼ï¼Œè§£å†³äº†ç°æœ‰æ¨¡å‹æ— æ³•åˆ©ç”¨relative rankingç›¸å¯¹æ’åä¿¡æ¯çš„ç¼ºç‚¹å’Œæ— æ³•å¤„ç†ç­‰å˜å˜æ¢çš„é£é™©ã€‚</td>
        <td>No-reference image quality assessment via transformers, relative ranking,and self-consistency</td>
        <td>307</td>
        <td>2021.08</td>
        <td>NR</td>
        <td>G</td>
        <td></td>
        <td> transformers, relative ranking, and self-consistency</td>
    </tr>
    <tr>
        <td>Maniqa</td>
        <td>ä¸€ç§æ–°çš„ç”±ViT, swin-transformeræ‰åˆè€Œæˆçš„ã€å¯ä»¥è€ƒè™‘ä¸åŒé€šé“é—´ä¿¡æ¯çš„NR-IQAæ–¹æ³•: MANIQA</td>
        <td>Maniqa: Muli-dimension attention network for no-reference image quality assessment</td>
        <td>301</td>
        <td>2022.04</td>
        <td>NR</td>
        <td>G</td>
        <td>swin transformer, ViT</td>
        <td></td>
    </tr>
    <tr>
        <td>TRIQ</td>
        <td>åœ¨ä½¿ç”¨CNNè¿›è¡Œå›¾åƒè´¨é‡è¯„ä¼°æ—¶ï¼Œå¾€å¾€éœ€è¦å›ºå®šè¾“å…¥ï¼Œæ— æ³•å¤„ç†å¤šç§åˆ†è¾¨ç‡çš„å›¾åƒï¼Œå› æ­¤è¦å¯¹åˆå§‹å›¾åƒè¿›è¡Œç¼©æ”¾ï¼Œä»è€Œä½¿å¾—åŸå§‹ä¿¡æ¯ä¸¢å¤±ï¼Œè€Œtransformerå¯ä»¥å¤„ç†ä¸åŒåˆ†è¾¨ç‡çš„å›¾åƒï¼Œå› æ­¤ç”¨äºè¯„ä¼°å›¾åƒè´¨é‡éå¸¸æœ‰æ•ˆã€‚</td>
        <td>Transformer for Image Quality Assessment</td>
        <td>230</td>
        <td>2020.12</td>
        <td>NR</td>
        <td>G</td>
        <td>ViT</td>
        <td></td>
    </tr>
    <tr>
        <td> IQT</td>
        <td>ä½¿ç”¨Siameseæ¶æ„æå–reference imageå’Œdistorted imageçš„ç‰¹å¾ï¼Œå¹¶ä½¿ç”¨ViTè¯„ä¼°è´¨é‡</td>
        <td>Perceptual Image Quality Assessment With Transformers</td>
        <td>167</td>
        <td>2021.04</td>
        <td>FR</td>
        <td>G</td>
        <td>Vision transformer(ViT)+Siamese</td>
        <td></td>
    </tr>
    <tr>
        <td>NTIRE 2021 Challenge on Perceptual IQA</td>
        <td></td>
        <td>NTIRE 2021 Challenge on Perceptual Image Quality Assessment</td>
        <td>114</td>
        <td>2021.05</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>NTIRE 2022 Challenge on Perceptual IQA</td>
        <td></td>
        <td>NTIRE 2022 Challenge on Perceptual Image Quality Assessment</td>
        <td>113</td>
        <td>2022.06</td>
        <td></td>
        <td></td>
        <td>ViT+CNN</td>
        <td></td>
    </tr>
    <tr>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td colspan="9" align="center">Framework-Based Method</td>
    </tr>
    <tr>
        <td>QAC</td>
        <td>ä¸éœ€è¦äººç±»çš„æ‰“åˆ†å³å¯å®ŒæˆIQA</td>
        <td>Learning without human scores for blind image quality assessment</td>
        <td>470</td>
        <td>2013.06</td>
        <td>NR</td>
        <td>G</td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>MetalQA</td>
        <td>ä¸€ç§ä½¿ç”¨å…ƒå­¦ä¹ è§£å†³NR-IQAä»»åŠ¡æ³›åŒ–é—®é¢˜çš„æ–¹æ³•</td>
        <td>MetalQA: Deep Meta-Learning for No-Reference Image Quality Assessment</td>
        <td>405</td>
        <td>2020.04</td>
        <td>NR</td>
        <td>G</td>
        <td>Meta-Learning</td>
        <td></td>
    </tr>
    <tr>
        <td>112</td>
        <td>DNN methods for BIQA;<br>DNN-based BIQA using deep features and quality prediction together;<br>predicting image quality scores;<br>The patch-input methods;<br>FR as patch label;<br>è¿™ä¸ªå·¥ä½œå¾ˆè´¼ï¼Œä½¿ç”¨çš„patchçš„labelæ˜¯error map</td>
        <td>Deep CNN-based blind image quality predictor</td>
        <td>315</td>
        <td>2018.06</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>UNIQUE</td>
        <td>UNIQUEæ˜¯ä¸€ä¸ªå…·æœ‰cross-distortion-scenario generalizationèƒ½åŠ›çš„BIQAæ¨¡å‹ï¼Œå¹¶ä¸”æå‡ºa method of training it on multiple IQA databases simultaneouslyï¼Œä»è€Œå¯ä»¥åœ¨è®­ç»ƒä¸€ç§distortionä¸Šçš„æ¨¡å‹æ—¶ï¼Œä½¿ç”¨å…¶ä»–distortionç±»å‹çš„æ•°æ®ã€‚</td>
        <td>Uncertainty-Aware Blind Image Quality Assessment in the Laboratory and Wild</td>
        <td>286</td>
        <td>2021.03</td>
        <td>NR</td>
        <td>G</td>
        <td>UNIQUE</td>
        <td></td>
    </tr>
    <tr>
        <td>Hallucinated-IQA</td>
        <td>åˆ©ç”¨è´¨é‡æ„ŸçŸ¥ç”Ÿæˆç½‘ç»œç”Ÿæˆé«˜åˆ†è¾¨ç‡çš„è™šå¹»å›¾åƒï¼Œå¹¶ç»“åˆå¯¹æ¯”å›¾åƒä¸è™šå¹»å›¾åƒä¹‹é—´çš„å·®å¼‚å›¾æ¥è®­ç»ƒè´¨é‡å›å½’ç½‘ç»œ</td>
        <td>Hallucinated-IQA: No-reference image quality assessment via adversarial learning</td>
        <td>273</td>
        <td>2018.04</td>
        <td>NR</td>
        <td>G</td>
        <td>adversarial learning</td>
        <td>CNN</td>
    </tr>
    <tr>
        <td>CONTRIQUE</td>
        <td>å¼€å‘åŸºäºCNNçš„IQAæ¨¡å‹çš„ä¸€ä¸ªéšœç¢æ˜¯ç¼ºä¹è¶³å¤Ÿå¤§çš„æ ‡è®°IQAæ•°æ®é›†ã€‚æ‰€ä»¥å¤§å¤šæ•°åŸºäºCNNçš„IQAæ¨¡å‹éƒ½åˆ©ç”¨äº†è¿ç§»å­¦ä¹ ï¼Œå³CNNåœ¨åƒImageNetè¿™æ ·çš„å¤§å‹æ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œç„¶åå¯¹å…·æœ‰ä¸»è§‚è´¨é‡åˆ¤æ–­çš„å›¾åƒè¿›è¡Œç«¯åˆ°ç«¯ç²¾ç»†è°ƒæ•´ã€‚<br>å°½ç®¡ç»è¿‡å¾®è°ƒçš„æ¨¡å‹åœ¨åˆæˆå¤±çœŸå’ŒçœŸå®å¤±çœŸæ–¹é¢éƒ½å–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„æ€§èƒ½ï¼Œä½†å¾®è°ƒéœ€è¦ä»”ç»†é€‰æ‹©è¶…å‚æ•°ï¼Œè¿™äº›è¶…å‚æ•°å¯èƒ½éšä¸åŒçš„IQAæ•°æ®åº“è€Œå˜åŒ–ã€‚æ­¤å¤–ï¼Œè¿‡åº¦çš„å¾®è°ƒä¼šä½¿æ¨¡å‹å¯¹è®­ç»ƒæ•°æ®è¿‡æ‹Ÿåˆï¼Œé™åˆ¶äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚<br>è¯¥æ–‡å¼•å…¥äº†ä¸€ä¸ªåŸºäºå¯¹æ¯”å­¦ä¹ çš„IQAè®­ç»ƒæ¡†æ¶ï¼Œæ—¨åœ¨ä½¿ç”¨æ— æ ‡è®°æ•°æ®é›†è·å¾—é«˜æ•ˆçš„å›¾åƒè´¨é‡è¡¨ç¤ºã€‚<br>è¯¥æ–‡ä½¿ç”¨ç•¸å˜ç±»å‹å’Œç¨‹åº¦çš„é¢„æµ‹ä½œä¸ºè¾…åŠ©ä»»åŠ¡ï¼Œä»åŒ…å«åˆæˆå’ŒçœŸå®æ··åˆå¤±çœŸçš„æœªæ ‡è®°å›¾åƒæ•°æ®é›†å­¦ä¹ ç‰¹å¾ã€‚ç„¶åè®­ç»ƒä¸€ä¸ªæ·±åº¦å·ç§¯ç¥ç»ç½‘ç»œ(CNN)ä½¿ç”¨ä¸€ä¸ªcontrastive pairwiseæ¥è§£å†³è¾…åŠ©é—®é¢˜ã€‚<br>ç„¶åå°†æå‡ºçš„è®­ç»ƒæ¡†æ¶å’Œäº§ç”Ÿçš„æ·±å±‚IQAæ¨¡å‹ä½œä¸ºå¯¹æ¯”å›¾åƒè´¨é‡è¯„ä¼°å™¨(CONTRIQUE)ã€‚åœ¨è¯„ä¼°æœŸé—´ï¼ŒCNNæƒé‡è¢«å†»ç»“ï¼Œçº¿æ€§å›å½’å™¨å°†å­¦ä¹ çš„è¡¨ç¤ºæ˜ å°„åˆ°NRè®¾ç½®ä¸­çš„è´¨é‡åˆ†æ•°ã€‚é€šè¿‡å¤§é‡çš„å®éªŒè¡¨æ˜ï¼Œä¸æœ€å…ˆè¿›çš„NRå›¾åƒè´¨é‡æ¨¡å‹ç›¸æ¯”ï¼ŒCONTRIQUEè·å¾—äº†å…·æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ï¼Œå³ä½¿æ²¡æœ‰ä»»ä½•é¢å¤–çš„CNNéª¨å¹²å¾®è°ƒã€‚<br>å­¦ä¹ åçš„è¡¨å¾å…·æœ‰é«˜åº¦çš„é²æ£’æ€§ï¼Œå¹¶ä¸”åœ¨å—åˆæˆæˆ–çœŸå®æ‰­æ›²å½±å“çš„å›¾åƒä¸­å…·æœ‰å¾ˆå¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚ç»“æœè¡¨æ˜ï¼Œä¸éœ€è¦å¤§é‡æ ‡æ³¨ä¸»è§‚å›¾åƒè´¨é‡æ•°æ®é›†ï¼Œå°±å¯ä»¥è·å¾—å…·æœ‰æ„ŸçŸ¥ç›¸å…³æ€§çš„å¼ºå¤§è´¨é‡è¡¨å¾</td>
        <td>Image quality assessment using contrastive learning</td>
        <td>206</td>
        <td>2022.06</td>
        <td>NR</td>
        <td>G</td>
        <td>contrastive learning, self-supervised learning</td>
        <td></td>
    </tr>
    <tr>
        <td>DeepFL-IQA</td>
        <td>DeepFL-IQAæå‡ºä¸€ç§åŸºäºweak supervision learningçš„æ–¹æ³•ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä½¿ç”¨å¤§é‡çš„å®¢è§‚IQAæŒ‡æ ‡æ‰“å‡ºçš„åˆ†æ•°ä½œä¸ºå‚è€ƒï¼Œå†ä½¿ç”¨ä¸»è§‚è´¨é‡åˆ†æ•°ä½œä¸ºå‚è€ƒè¿›è¡Œå¾®è°ƒï¼Œä»¥è·å¾—é¢„æµ‹å‡ºç¬¦åˆHVSåˆ†æ•°çš„èƒ½åŠ›ã€‚</td>
        <td>DeepFL-IQA: Weak Supervision for Deep IQA Feature Learning</td>
        <td>67</td>
        <td>2020.01</td>
        <td>NR</td>
        <td>G</td>
        <td>Weak Supervision Learning</td>
        <td>CNN</td>
    </tr>
    <tr>
        <td>CVRDK-IQA</td>
        <td>æ¯”èµ·NRæ–¹æ³•ï¼ŒFRæ–¹æ³•æ•ˆæœå¾€å¾€æ›´å¥½ï¼Œæœ¬æ–‡åˆ©ç”¨çŸ¥è¯†è’¸é¦ï¼Œèƒ½å¤ŸæŠŠFRå­¦åˆ°çš„ä¸œè¥¿æ•™ç»™NRï¼šå°† FR-teacher ä¸­çš„çŸ¥è¯†ä¼ é€’ç»™ NAR-studentï¼Œä»è€Œä½¿ NAR-student èƒ½å¤Ÿåœ¨æ²¡æœ‰pixel levelå¯¹é½çš„å‚è€ƒå›¾åƒçš„æƒ…å†µä¸‹è¿›è¡Œå›¾åƒè´¨é‡è¯„ä¼°ã€‚ä¸¤ç»„æ¨¡å‹ç»“æ„å®Œå…¨ç›¸åŒã€‚<br>è™½ç„¶æˆ‘æ²¡æœ‰å‚è€ƒå›¾åƒï¼Œä½†æˆ‘çŸ¥é“å‚è€ƒå›¾åƒåº”è¯¥æ˜¯ä»€ä¹ˆæ ·çš„</td>
        <td>Content-Variant Reference Image Quality Assessment via Knowledge Distillation</td>
        <td>34</td>
        <td>2022.02</td>
        <td>NR</td>
        <td>G</td>
        <td>Knowledge Distillation</td>
        <td></td>
    </tr>
    <tr>
        <td>CNN-Based Medical Ultrasound IQA</td>
        <td>é‰´äºè¶…å£°å›¾åƒæ ·æœ¬ä¸ä¸°å¯Œï¼Œå¼•å…¥è¿ç§»å­¦ä¹ ç­–ç•¥ã€‚åˆ©ç”¨å…‰å­¦å›¾åƒæ•°æ®é›†å¯¹CNNæ¨¡å‹è¿›è¡Œé¢„è®­ç»ƒï¼Œç„¶åå†ç”¨è¶…å£°å›¾åƒè¿›è¡Œå¾®è°ƒï¼Œæœ‰æ•ˆè§£å†³äº†è¶…å£°å›¾åƒæ•°æ®æœ‰é™å¯¼è‡´çš„è¿‡æ‹Ÿåˆé—®é¢˜ã€‚</td>
        <td>CNN-Based Medical Ultrasound Image Quality Assessment</td>
        <td>28</td>
        <td>2021.07</td>
        <td>NR</td>
        <td>G</td>
        <td>transfer learning</td>
        <td></td>
    </tr>
    <tr>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td colspan="9" align="center">Medical IQA</td>
    </tr>
    <tr>
        <td>164</td>
        <td>åœ¨è„‘æˆåƒç ”ç©¶ä¸­ï¼Œæ•°æ®è´¨é‡æ˜¯ä¸€ä¸ªé‡è¦çš„æ··æ‚å› ç´ ï¼Œå°¤å…¶æ˜¯åœ¨è„‘å‘è‚²ç ”ç©¶ä¸­ï¼Œå¹´é¾„ä¸æ‰«æä»ªå†…çš„è¿åŠ¨å’Œæ•°æ®è´¨é‡ç³»ç»Ÿç›¸å…³ã€‚è¯¥è®ºæ–‡ç ”ç©¶è¯„ä¼°äº†å¤šç§å®šé‡å›¾åƒè´¨é‡æŒ‡æ ‡ï¼ŒåŒ…æ‹¬Preprocessed Connectomes Project&#39;s Quality Assurance Protocol (QAP)æä¾›çš„æŒ‡æ ‡å’ŒFreeSurferæä¾›çš„Euler numberã€‚<br>åœ¨å¤šç§è´¨é‡æŒ‡æ ‡ä¸­ï¼ŒEuler numberè¡¨ç°æœ€å¥½ã€‚åŸå› åœ¨äºæŸäº›æŒ‡æ ‡ï¼ˆå¦‚ä¿¡å™ªæ¯”ï¼ˆSNRï¼‰å’Œå¯¹æ¯”å™ªå£°æ¯”ï¼ˆCNRï¼‰ï¼‰è¯„ä¼°çš„æ˜¯å›¾åƒçš„å…¨å±€ç‰¹æ€§ï¼Œè€ŒEuleræ•°è¯„ä¼°çš„æ˜¯å±€éƒ¨çš„æ‹“æ‰‘ç»“æ„ã€‚å±€éƒ¨çš„è¿åŠ¨ä¼ªå½±æˆ–é‡å»ºé”™è¯¯å¯èƒ½å¯¹å…¨å±€æŒ‡æ ‡çš„å½±å“è¾ƒå°ï¼Œä½†å¯¹Euleræ•°çš„å½±å“è¾ƒå¤§ã€‚ä¾‹å¦‚ï¼Œä¸€ä¸ªå±€éƒ¨çš„è¿åŠ¨ä¼ªå½±å¯èƒ½å¯¼è‡´çš®å±‚è¡¨é¢å‡ºç°å­”æ´æˆ–æ–­è£‚ï¼Œä»è€Œæ˜¾è‘—å½±å“Euleræ•°ï¼Œä½†å¯¹SNRæˆ–CNRçš„å½±å“å¯èƒ½ä¸æ˜æ˜¾ã€‚<br>è¿™è¡¨æ˜åœ¨å®é™…åº”ç”¨ä¸­ï¼Œé€‰æ‹©åˆé€‚çš„è´¨é‡è¯„ä¼°æŒ‡æ ‡éœ€è¦æ ¹æ®å…·ä½“çš„ç ”ç©¶éœ€æ±‚å’Œæ•°æ®ç‰¹ç‚¹æ¥å†³å®šã€‚</td>
        <td>Quantitative assessment of structural image quality</td>
        <td>360</td>
        <td>2018.04</td>
        <td>NR</td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td colspan="9" align="center">IQA for Dehazing Algorithm</td>
    </tr>
    <tr>
        <td>165</td>
        <td>å›¾åƒå»é›¾ç®—æ³•ï¼ˆDehazing Algorithmsï¼Œç®€ç§°DHAsï¼‰æ˜¯ä¸€ç±»ç”¨äºæ”¹å–„åœ¨é›¾éœ¾ç­‰æ¶åŠ£å¤©æ°”æ¡ä»¶ä¸‹æ‹æ‘„çš„å›¾åƒè´¨é‡çš„ç®—æ³•ã€‚è¿™äº›ç®—æ³•çš„ç›®æ ‡æ˜¯å»é™¤å›¾åƒä¸­çš„é›¾æ°”ï¼Œæ¢å¤å›¾åƒçš„æ¸…æ™°åº¦ã€å¯¹æ¯”åº¦å’Œè‰²å½©ï¼Œä»è€Œæé«˜å›¾åƒçš„è§†è§‰è´¨é‡å’Œå¯ç”¨æ€§ã€‚<br>ç°æœ‰çš„FR IQAæ–¹æ³•ä¸é€‚ç”¨äºDHAçš„è¯„ä¼°ï¼Œä¸»è¦æ˜¯å› ä¸ºå»é›¾è¿‡ç¨‹ä¸ä»…ä»…æ˜¯å›¾åƒæ¢å¤ï¼Œè¿˜æ¶‰åŠåˆ°å¯¹æ¯”åº¦å¢å¼ºã€è‰²å½©è°ƒæ•´å’Œç»“æ„æ¢å¤ç­‰å¤šä¸ªæ–¹é¢ã€‚è¿™äº›ç‰¹æ€§ä½¿å¾—ä¼ ç»Ÿçš„FR IQAæ–¹æ³•åœ¨è¯„ä¼°å»é›¾æ•ˆæœæ—¶å­˜åœ¨å±€é™æ€§ã€‚<br>æœ¬æ–‡æå‡ºçš„å»é›¾ç®—æ³•è¯„ä¼°æ–¹æ³•é€šè¿‡ç»¼åˆè€ƒè™‘å›¾åƒç»“æ„æ¢å¤Image Structure Recoveringã€è‰²å½©å†ç°Color Renditionå’Œä½å¯¹æ¯”åº¦åŒºåŸŸçš„è¿‡åº¦å¢å¼ºOver-Enhancement of Low-Contrast Areasï¼Œèƒ½å¤Ÿæ›´å…¨é¢åœ°è¯„ä¼°å»é›¾ç®—æ³•çš„æ•ˆæœã€‚è¯¥æ–¹æ³•ä¸ä»…é€‚åº”äº†å»é›¾è¿‡ç¨‹ä¸­çš„å¯¹æ¯”åº¦å¢å¼ºå’Œè‰²å½©è°ƒæ•´ï¼Œè¿˜é€šè¿‡é’ˆå¯¹èˆªæ‹å›¾åƒçš„æ”¹è¿›ï¼Œæé«˜äº†è¯„ä¼°æ–¹æ³•çš„é€‚ç”¨æ€§å’Œå‡†ç¡®æ€§ã€‚</td>
        <td>Quality evaluation of image dehazing methods using synthetic hazy images</td>
        <td>222</td>
        <td>2019.02</td>
        <td>FR</td>
        <td>dehazing</td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td colspan="9" align="center">Portrait Quality Assessment</td>
    </tr>
    <tr>
        <td>166</td>
        <td>éšç€æ™ºèƒ½æ‰‹æœºçš„æ™®åŠï¼Œè‚–åƒæ‘„å½±å˜å¾—æ™®éï¼Œä½†å®ç°ä¸“ä¸šè´¨é‡çš„å›¾åƒä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚<br>åœ¨è‚–åƒä¸­ï¼Œé¢éƒ¨åŒºåŸŸé€šå¸¸æ˜¯è§†è§‰ç„¦ç‚¹ï¼Œå…¶è´¨é‡å¯¹æ•´ä½“è‚–åƒè´¨é‡çš„å½±å“è¿œå¤§äºå…¶ä»–åŒºåŸŸã€‚ä¼ ç»ŸIQAæ–¹æ³•é€šå¸¸å¯¹æ•´å¹…å›¾åƒè¿›è¡Œå…¨å±€è¯„ä¼°ï¼Œéš¾ä»¥æ•æ‰åˆ°é¢éƒ¨åŒºåŸŸçš„ç‰¹æ®Šé‡è¦æ€§ã€‚åŒæ—¶ï¼Œè‚–åƒè´¨é‡ä¸ä»…å–å†³äºé¢éƒ¨åŒºåŸŸçš„è´¨é‡ï¼Œè¿˜ä¸èƒŒæ™¯ã€æ„å›¾ç­‰å…¨å±€å› ç´ å¯†åˆ‡ç›¸å…³ã€‚ä¼ ç»ŸIQAæ–¹æ³•å¾€å¾€éš¾ä»¥åŒæ—¶å…¼é¡¾å±€éƒ¨å’Œå…¨å±€çš„è´¨é‡è¯„ä¼°ã€‚å› æ­¤éœ€è¦æå‡ºå¼€å‘ä¸“é—¨çš„è‚–åƒè´¨é‡è¯„ä¼°æ–¹æ³•ã€‚<br>NTIRE 2024 Challengeâ€”â€”Deep Portrait Quality Assessmentä¸­çš„å‚èµ›é˜Ÿä¼çº·çº·æå‡ºäº†è‡ªå·±çš„æ–¹æ³•ï¼Œä¾‹å¦‚PQEæå‡ºäº†ä¸€ç§ä¸¤åˆ†æ”¯è‚–åƒè´¨é‡è¯„ä¼°æ¨¡å‹ï¼Œåˆ†åˆ«å¯¹å…¨å›¾å’Œé¢éƒ¨ç»„ä»¶è¿›è¡Œå»ºæ¨¡ï¼Œå¹¶ä½¿ç”¨LIQEæå–åœºæ™¯å’Œè´¨é‡ç‰¹å¾ï¼›SARæå‡ºäº†ä¸€ç§åœºæ™¯è‡ªé€‚åº”å…¨å±€ä¸Šä¸‹æ–‡å’Œå±€éƒ¨é¢éƒ¨æ„ŸçŸ¥ç½‘ç»œï¼Œé€šè¿‡é¢éƒ¨æ£€æµ‹å™¨ç²¾ç¡®å®šä½é¢éƒ¨åŒºåŸŸï¼Œå¹¶ä½¿ç”¨Vision Transformerå¯¹å±€éƒ¨é¢éƒ¨åŒºåŸŸå’Œå…¨å±€å›¾åƒè¿›è¡Œå»ºæ¨¡ã€‚</td>
        <td>Deep Portrait Quality Assessment. A NTIRE 2024 Challenge Survey</td>
        <td>15</td>
        <td>2024.04</td>
        <td>NR</td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td colspan="9" align="center">IQA for Low Light Enhancement</td>
    </tr>
    <tr>
        <td>NLIEE</td>
        <td>ä½å…‰ç…§å›¾åƒå¢å¼ºç®—æ³•ï¼ˆLow-Light Image Enhancement Algorithmsï¼ŒLIEAsï¼‰æ˜¯ä¸€ç±»ä¸“é—¨ç”¨äºæ”¹å–„åœ¨ä½å…‰ç…§æ¡ä»¶ä¸‹æ‹æ‘„çš„å›¾åƒè´¨é‡çš„ç®—æ³•ã€‚è¯¥ç®—æ³•éœ€è¦æé«˜å›¾åƒçš„æ•´ä½“äº®åº¦ï¼Œè°ƒæ•´å›¾åƒçš„å¯¹æ¯”åº¦ï¼Œå‡å°‘å›¾åƒä¸­çš„å™ªå£°ï¼Œè°ƒæ•´å›¾åƒçš„è‰²å½©ã€‚<br>ä¸ºäº†å‡†ç¡®è¯„ä¼°ä½å…‰ç…§å›¾åƒå¢å¼ºç®—æ³•çš„æ•ˆæœï¼Œéœ€è¦ä¸€ç§èƒ½å¤Ÿç»¼åˆè€ƒè™‘å¤šç§å¤±çœŸç±»å‹å’Œå¤æ‚æ€§çš„è¯„ä¼°æ–¹æ³•ã€‚è¿™ç§è¯„ä¼°æ–¹æ³•éœ€è¦èƒ½å¤Ÿä»äº®åº¦å¢å¼ºã€è‰²å½©å¯¹æ¯”ã€å™ªå£°æ°´å¹³å’Œç»“æ„å®Œæ•´æ€§ç­‰å¤šä¸ªç»´åº¦è¯„ä¼°å›¾åƒè´¨é‡ã€‚<br>NLIEEæ˜¯é¦–ä¸ªä¸“é—¨ä¸ºä½å…‰ç…§å›¾åƒå¢å¼ºè´¨é‡è¯„ä¼°è®¾è®¡çš„æ— å‚è€ƒè¯„ä¼°æŒ‡æ ‡ï¼Œå…¶ä»å››ä¸ªå…³é”®æ–¹é¢æå–ç‰¹å¾ï¼Œå…¨é¢è¯„ä¼°ä½å…‰ç…§å›¾åƒå¢å¼ºåçš„è´¨é‡ï¼šå…‰ç…§å¢å¼ºï¼Œè‰²å½©å¯¹æ¯”ï¼Œå™ªå£°æµ‹é‡ï¼Œç»“æ„è¯„ä¼°ã€‚ä¸é€šç”¨çš„è¯„ä¼°æ–¹æ³•ç›¸æ¯”ï¼Œå…·æœ‰æ›´é«˜çš„å‡†ç¡®æ€§å’Œç›¸å…³æ€§ã€‚</td>
        <td>A no-reference evaluation metric for low-light image enhancement</td>
        <td>55</td>
        <td>2021.06</td>
        <td>NR</td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td colspan="9" align="center">Specific Distortion</td>
    </tr>
    <tr>
        <td>133</td>
        <td>NRç±»å‹ï¼Œä»…å°†é”åº¦ä½œä¸ºä¸€ä¸ªæŒ‡æ ‡ï¼Œ<br>JNB comes from just noticeable difference(JND) and is the minimum difference inintensity value relative to background that is noticeable.JNB is the minimum perceivable blur around an edge givenahighercontrastthanaJND</td>
        <td>A no-reference objective image sharpness metric based on the notion of just noticeable blur (JNB)</td>
        <td>1004</td>
        <td>2009.04</td>
        <td>NR</td>
        <td>blur distortion</td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>BIBLE</td>
        <td>ç‰¹å®šäºæŸä¸€ç±»å‹(distortion specific approach)çš„IQAæ–¹æ³•ï¼Œéœ€è¦ä¸€äº›å…ˆéªŒçŸ¥è¯†ï¼Œé‡åˆ°æœªçŸ¥æ¥æºçš„distotionå°±æ— èƒ½ä¸ºåŠ›ã€‚</td>
        <td>No-reference image blur assessment based on discrete orthogonal moments</td>
        <td>280</td>
        <td>2015.01</td>
        <td>NR</td>
        <td>blur distortion</td>
        <td>ç¦»æ•£TchebichefçŸ©</td>
        <td></td>
    </tr>
    <tr>
        <td>169</td>
        <td>å›¾åƒå»æ¨¡ç³Šï¼ˆImage Deblurringï¼‰æ˜¯è®¡ç®—æœºè§†è§‰å’Œå›¾åƒå¤„ç†é¢†åŸŸçš„ä¸€ä¸ªé‡è¦ä»»åŠ¡ï¼Œæ—¨åœ¨ä»æ¨¡ç³Šå›¾åƒä¸­æ¢å¤å‡ºæ¸…æ™°çš„å›¾åƒå†…å®¹ã€‚æ¨¡ç³Šå¯èƒ½ç”±å¤šç§å› ç´ å¼•èµ·ï¼Œå¦‚ç›¸æœºæŠ–åŠ¨ã€ç‰©ä½“è¿åŠ¨ã€ç„¦ç‚¹ä¸å‡†ç¡®æˆ–å¤§æ°”æ¹æµç­‰ã€‚å›¾åƒå»æ¨¡ç³Šçš„ç›®æ ‡æ˜¯é€šè¿‡ç®—æ³•æ¶ˆé™¤è¿™äº›æ¨¡ç³Šï¼Œæ¢å¤å‡ºåŸå§‹çš„æ¸…æ™°å›¾åƒã€‚<br>å»æ¨¡ç³Šç®—æ³•çš„ç›®æ ‡æ˜¯æ¢å¤å‡ºæ¸…æ™°çš„å›¾åƒï¼Œä½†å®é™…ä¸­å¾ˆéš¾å®Œå…¨æ¢å¤å‡ºç†æƒ³çš„æ¸…æ™°å›¾åƒã€‚å»æ¨¡ç³Šç®—æ³•å¯èƒ½å¼•å…¥å„ç§ä¼ªå½±ï¼Œè¿™äº›ä¼ªå½±å¯¹å›¾åƒçš„æ„ŸçŸ¥è´¨é‡æœ‰æ˜¾è‘—å½±å“ã€‚ä¾‹å¦‚ï¼ŒæŒ¯é“ƒä¼ªå½±ï¼ˆåœ¨è¾¹ç¼˜é™„è¿‘å‡ºç°çš„æ³¢çº¹çŠ¶ç»“æ„ï¼‰æ˜¯å»æ¨¡ç³Šç®—æ³•ä¸­æœ€å¸¸è§çš„ä¼ªå½±ä¹‹ä¸€ï¼Œå®ƒå¯¹äººç±»è§†è§‰ç³»ç»Ÿçš„å¹²æ‰°éå¸¸å¤§ã€‚<br>ä¸åŒçš„å»æ¨¡ç³Šç®—æ³•å¯èƒ½äº§ç”Ÿä¸åŒç±»å‹çš„ä¼ªå½±ï¼Œè¿™äº›ä¼ªå½±çš„ç‰¹å¾å’Œå½±å“æ–¹å¼å„ä¸ç›¸åŒã€‚å› æ­¤ï¼Œéœ€è¦ä¸€ç§èƒ½å¤Ÿä¸“é—¨é’ˆå¯¹è¿™äº›ä¼ªå½±è¿›è¡Œè¯„ä¼°çš„æ–¹æ³•ã€‚<br>è¯¥ç®—æ³•è®¾è®¡äº†ä¸€ç³»åˆ—ä¸“é—¨é’ˆå¯¹å»æ¨¡ç³Šä¼ªå½±çš„ç‰¹å¾ï¼ŒåŒ…æ‹¬ï¼š<br>æå‡ºäº†ä¸€ç§æ–°çš„æ— å‚è€ƒæ–¹æ³•ï¼ˆPyrRingï¼‰æ¥æ£€æµ‹å¤§è§„æ¨¡æŒ¯é“ƒä¼ªå½±ï¼›ä½¿ç”¨å¤šç§æ–¹æ³•æ¥è¯„ä¼°å»æ¨¡ç³Šç»“æœä¸­çš„å™ªå£°æ°´å¹³ï¼›ä½¿ç”¨å¤šç§é”åº¦åº¦é‡æ–¹æ³•æ¥è¯„ä¼°å»æ¨¡ç³Šç»“æœçš„æ¸…æ™°åº¦ã€‚è¿™äº›ç‰¹å¾èƒ½å¤Ÿå…¨é¢è¯„ä¼°å»æ¨¡ç³Šç»“æœçš„è´¨é‡ï¼Œè€Œä¸ä»…ä»…æ˜¯å•ä¸€ç±»å‹çš„ä¼ªå½±ã€‚</td>
        <td>A no-reference metric for evaluating the quality of motion deblurring</td>
        <td>127</td>
        <td>2013.11</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>168</td>
        <td>åœ¨JPEGå‹ç¼©è¿‡ç¨‹ä¸­ï¼Œå—æ•ˆåº”ï¼ˆBlocking Effectï¼‰å’Œæ¨¡ç³Šæ•ˆåº”ï¼ˆBlurring Effectï¼‰æ˜¯éœ€è¦é‡ç‚¹è§£å†³çš„é—®é¢˜ï¼Œæœ¬æ–¹æ³•é’ˆå¯¹è¿™ä¸¤ç§å›¾åƒé€€åŒ–æå‡ºäº†è¯„ä¼°JPEGå‹ç¼©å›¾åƒè´¨é‡çš„æ–¹æ³•ï¼Œæœ‰æ•ˆæ•æ‰äº†è¿™äº›distortionï¼Œèµ·åˆ°äº†æ¯”é€šç”¨IQAæ–¹æ³•æ›´å¥½çš„æ•ˆæœã€‚</td>
        <td>Full reference image quality metrics for JPEG compressed images</td>
        <td>40</td>
        <td>2015.02</td>
        <td>NR</td>
        <td>JPEG</td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
         <td colspan="9" align="center">ä»¥ä¸‹åœ¨å…¶å®ƒéƒ¨åˆ†ä½¿ç”¨</td>
    </tr>
    <tr>
        <td> Single stimulus categorical rating(SIQA)</td>
        <td>æµ‹è¯•å›¾åƒä¼šåœ¨å±å¹•ä¸Šæ˜¾ç¤ºä¸€æ®µæ—¶é—´ï¼Œä¹‹åå›¾åƒæ¶ˆå¤±ï¼Œè§‚å¯Ÿè€…éœ€è¦æ ¹æ®ä¸€ä¸ªæŠ½è±¡çš„äº”åˆ†ç±»å°ºåº¦ï¼ˆä¼˜ç§€ã€è‰¯å¥½ã€ä¸­ç­‰ã€å·®ã€æå·®ï¼‰å¯¹å›¾åƒè´¨é‡è¿›è¡Œè¯„åˆ†ã€‚</td>
        <td>Methodology for the subjective assessment of the quality of television pictures</td>
        <td>1004</td>
        <td>2002ï¼ˆå…·ä½“æœˆä»½æ²¡æœ‰æ‰¾åˆ°ï¼‰</td>
        <td>FR</td>
        <td>G</td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>Double stimulus categorical rating(SIQA)</td>
        <td>ä¸å•åˆºæ¿€åˆ†ç±»è¯„åˆ†æ–¹æ³•ç±»ä¼¼ï¼Œä¸åŒä¹‹å¤„åœ¨äºï¼Œè¿™ç§æ–¹æ³•ä¼šåŒæ—¶æ˜¾ç¤ºæµ‹è¯•å›¾åƒå’Œå‚è€ƒå›¾åƒä¸€æ®µæ—¶é—´ã€‚ä¹‹åï¼Œå›¾åƒæ¶ˆå¤±ï¼Œè§‚å¯Ÿè€…éœ€è¦æ ¹æ®ä¹‹å‰æåˆ°çš„äº”åˆ†ç±»å°ºåº¦å¯¹æµ‹è¯•å›¾åƒçš„è´¨é‡è¿›è¡Œè¯„åˆ†</td>
        <td>Methodology for the subjective assessment of the quality of television pictures</td>
        <td>1004</td>
        <td>2003ï¼ˆå…·ä½“æœˆä»½æ²¡æœ‰æ‰¾åˆ°ï¼‰</td>
        <td>FR</td>
        <td>G</td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>157</td>
        <td>ä»aesthetic point of viewå‡ºå‘ï¼Œä»¥Composition, Lighting, Focus Controlling, Colorä¸ºæ ‡å‡†è¯„ä¼°å›¾åƒã€‚</td>
        <td>Photo and video quality evaluation: Focusing on the subject</td>
        <td>644</td>
        <td>2008.10</td>
        <td></td>
        <td>ç¾å­¦å›¾åƒ</td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>Reduced - Reference Image Quality Metric for Contrast Change(RIQMC)</td>
        <td>è¯„ä¼°å¯¹æ¯”åº¦å˜åŒ–é€ æˆçš„å›¾åƒè´¨é‡å˜åŒ–ï¼›<br>ä»ç›¸ä¼¼åº¦å’Œèˆ’é€‚åº¦ä¸¤ä¸ªè§’åº¦è¯„ä»·å›¾åƒè´¨é‡ï¼š<br>ç›¸ä¼¼åº¦ç”±ç†µçš„å·®å¼‚ã€ç›¸ä½ä¸€è‡´æ€§ç»„æˆï¼›<br>èˆ’é€‚åº¦ä½¿ç”¨å››é˜¶çš„ç»Ÿè®¡é‡ï¼ˆå‡å€¼ã€æ–¹å·®ã€ååº¦ã€å³°åº¦ï¼‰ã€‚<br>éšåå°†äºŒè€…çº¿æ€§èåˆã€‚</td>
        <td>The analysis of image contrast: From quality assessment to automatic enhancement</td>
        <td>405</td>
        <td>2015.01</td>
        <td>RR</td>
        <td>G</td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>OSVP for RR IQA</td>
        <td>é€šè¿‡æ¨¡ä»¿çš®è´¨ç»†èƒé—´ç›¸äº’ä½œç”¨æ¥æå– OSVP ç‰¹å¾(orientation-selectivity-based visual pattern)ç”¨äºè§†è§‰ä¿¡æ¯è¡¨ç¤ºã€‚<br>å…ˆè®¡ç®—åƒç´ æ¢¯åº¦æ–¹å‘ä½œä¸ºå…¶æ–¹å‘ï¼Œæ ¹æ®æ–¹å‘ç›¸ä¼¼æ€§ç¡®å®šåƒç´ é—´ç›¸äº’ä½œç”¨ç±»å‹ï¼ˆå…´å¥‹æˆ–æŠ‘åˆ¶ï¼‰ï¼Œç„¶ååŸºäºæ­¤è®¡ç®—åƒç´ çš„ OSVPï¼Œæœ€åå°†å…·æœ‰ç›¸åŒ OSVP çš„åƒç´ ç»„åˆï¼Œå°†å›¾åƒæ˜ å°„ä¸ºåŸºäº OSVP çš„ç›´æ–¹å›¾ã€‚<br>å„ç§å¤±çœŸä¼šæ”¹å˜å›¾åƒç»“æ„å’Œåƒç´ é—´ç©ºé—´ç›¸å…³æ€§ï¼Œå¯¼è‡´ OSVP ç±»å‹å˜åŒ–ã€‚é€šè¿‡è®¡ç®—å‚è€ƒå›¾åƒå’Œå¤±çœŸå›¾åƒåŸºäº OSVP ç›´æ–¹å›¾çš„ç›¸ä¼¼åº¦æ¥è¡¡é‡è´¨é‡é€€åŒ–ã€‚<br>è¯„ä»·ï¼šä½œè€…æ˜¾ç„¶æœ‰è„‘ç§‘å­¦èƒŒæ™¯ï¼</td>
        <td>Orientation selectivity based visual pattern for reduced-reference image quality assessment</td>
        <td>105</td>
        <td>2016.07</td>
        <td>RR</td>
        <td>G</td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>RR IQA using SDM</td>
        <td>å®šä¹‰åŸå§‹å’Œå¤±çœŸå›¾åƒçš„ç»“æ„é€€åŒ–ä¿¡æ¯ï¼ˆstructural degradation informationï¼‰ï¼Œé€šè¿‡å…¶è·ç¦»çš„éçº¿æ€§ç»„åˆæˆ– SVM ç§¯åˆ†å¼€å‘åŸºäº ç»“æ„é€€åŒ–æ¨¡å‹ structural degradation model çš„ RR å›¾åƒè´¨é‡åº¦é‡ã€‚</td>
        <td>A new reduced-reference image quality assessment using structural degradation model</td>
        <td>86</td>
        <td>2013.05</td>
        <td>RR</td>
        <td>G</td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>MLIQM</td>
        <td>MLIQM first classifies a distorted image into five scales using multi-support vector machine (SVM) classification according to the quality scale recommended by the ITU, and then uses an SVR to predict the final score of the distorted image<br>ç»å…¸æœºå™¨å­¦ä¹ æ–¹æ³•</td>
        <td>Machine learning to design full-reference image quality assessment algorithm</td>
        <td>67</td>
        <td></td>
        <td>FR</td>
        <td>G</td>
        <td></td>
        <td></td>
    </tr>
</table>
